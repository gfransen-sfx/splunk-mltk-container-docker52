{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splunk App for Data Science and Deep Learning - Hashing Encoder Distances: Distance For Tokenzied Strings from Reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashing Encoder Distances: Distance For Tokenzied Strings from Reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#from hashlib import md5\n",
    "import hashlib\n",
    "from base64 import encode\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - push data from Splunk\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. You utilize the `mode=stage` flag in the in the `| fit` command to do this. The search results are accessible then as csv file with the same model name that is defined in the `into app:<modelname>` part of the fit statement. Additionally, meta data is retrieved and accessible as json file. In the same way you can further work with the meta data object as it is exposed in the fit and apply function definitions below in stage 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "df, param = stage(\"hashing_encoder_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    model = {}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df,param)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model,df,param):\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "returns = fit(model,df,param)\n",
    "print(returns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "\n",
    "    \n",
    "    field_name = param['feature_variables'][0]\n",
    "    \n",
    "    comparison_string = param['options']['params']['comparison_string']\n",
    "    \n",
    "    try:\n",
    "        vector_length=int(param['options']['params']['vector_length'])\n",
    "    except:\n",
    "        vector_length=16\n",
    "        \n",
    "    try:\n",
    "        return_vector=int(param['options']['params']['return_vector'])\n",
    "    except:\n",
    "        return_vector=0\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        regex=param['options']['params']['tokenizing_regex']\n",
    "    except:\n",
    "        regex=r'[^\\w\\s]'\n",
    "    \n",
    "    # define the regex pattern used for tokenisation\n",
    "    pattern = re.compile(regex)\n",
    "    \n",
    "    \n",
    "    # Define a new row\n",
    "    new_row = {field_name: [comparison_string]}\n",
    "\n",
    "    # Append the row\n",
    "    df = pd.concat([df,pd.DataFrame(new_row)], ignore_index=True)\n",
    "    \n",
    "    # split results up using the regex pattern to create a list of tokens\n",
    "    df['token_list'] = df[field_name].astype(str).apply(lambda x: pattern.split(x))\n",
    "    # apply strip to each list item (token) to remove leading and trailing whitespace\n",
    "    df['token_list'] = df['token_list'].apply(lambda x: [item.strip() for item in x])\n",
    "    df['token_list'] = df['token_list'].apply(lambda row: [item for item in row if item != ''])\n",
    "\n",
    "    # Now that we have the tokens, we need to hash them and apply hashing trick to create an embedding vector\n",
    "    # define length of enbedding vector\n",
    "\n",
    "    # initialise a new vector of length embedding_len, defined as a new field in the query results\n",
    "    df['embedding'] = df.apply(lambda row: [0] * vector_length, axis=1)\n",
    "\n",
    "    # convert tokens to indexes into embedding array\n",
    "    df['index_list'] = df['token_list'].apply(lambda token_list: [int(hashlib.md5(token.encode()).hexdigest(), 16) % (vector_length) for token in token_list])\n",
    "    \n",
    "    for row in range(len(df['index_list'])):\n",
    "        for col in df['index_list'][row]:\n",
    "            df['embedding'][row][col] += 1\n",
    "            \n",
    "    encoded_df = df['embedding'].apply(pd.Series)\n",
    "    \n",
    "    row_to_subtract = encoded_df.iloc[-1]\n",
    "    encoded_df_diff = (encoded_df - row_to_subtract).drop(encoded_df.index[-1])\n",
    "    \n",
    "    distance = pd.DataFrame(np.sqrt((encoded_df_diff**2).sum(axis=1)))\n",
    "    distance.rename(columns={0: 'distance'},inplace=True)\n",
    "\n",
    "    if return_vector==1:\n",
    "        return encoded_df_diff\n",
    "    else:\n",
    "        return distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "result = apply(model,df,param)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
